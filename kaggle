```{r, install packages}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(BART)){install.packages("BART")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(glmnet)){install.packages("glmnet")}
if(!require(quantmod)){install.packages("quantmod")}
if(!require(tidyquant)){install.packages("tidyquant")}
if(!require(purrr)){install.packages("purrr")}
if(!require(dplyr)){install.packages("dplyr")}
if(!require(rpart)){install.packages(c("rpart","rpart.plot"))}
if(!require(randomForest)){install.packages(c("randomForest"))}
if(!require(doParallel)){install.packages(c("doParallel"))}
if(!require(caret)){install.packages(c("caret"))}
if(!require(dummies)){install.packages("dummies")}
if(!require(anomalize)){install.packages("anomalize")}

if(!require(reticulate)){install.packages("reticulate")}


library(caret) 
library(reticulate)
library(anomalize) 
library(dplyr) 
library(rpart) 
#library(rpart.plot)
library(tidyquant)                      
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
library(glmnet)                         # Package for penalized regressions
library(quantmod)
library(purrr)
library(glmnet)                                     # This is THE package for penalised regressions
library(tidyverse)                                  # ... the usual core packages
library(h2o)
```

#```{python}
#reticulate::repl_python()
# test
#import matplotlib.pyplot as plt
#import numpy as np
#import pandas as pd
#import tensorflow as tf
#from keras import backend as K
#```

```{r}
data = read.csv2(file = "CC_training.csv", sep = ';')

data_testing = read.csv2(file = "CC_testing.csv", sep = ';')

``` 

```{r first, warning = FALSE, message = FALSE}
#data <- coin_history %>% arrange(time_open,name)                 # Just making sure all is in order

cc_id = levels(as.factor(data$cc_id))                           # Set of assets


data <- data  %>% 
    group_by(cc_id)# %>%                           # Group asset by asset
    #na.omit()                                     # Take out missing data
features <- colnames(data[7:131])

data_testing <- data_testing  %>% 
    group_by(cc_id2) #%>%                           # Group asset by asset
    #na.omit()                                     # Take out missing data
    
```

```{r}
data <- data %>% 
    group_by(date) %>%                                   # Group by date
    mutate(RET_1D_C = RET_1D > median(RET_1D),        # Create the categorical labels
           RET_1M_C = RET_1M > median(RET_1M)) %>%
    ungroup() %>%
    mutate_if(is.logical, as.factor)

data_train_scaled = data  %>% ungroup() %>% select(-c(cc_id, date, RET_1D,RET_1W,RET_1M,RET_3M, RET_1D_C, RET_1M_C)) %>% scale() #%>%  as.data.frame()

data_testing_scaled = data_testing %>% ungroup() %>% select(-c(cc_id2, date_id, Id)) %>% scale()# %>% as.data.frame()

data_test_scaled = scale(data_testing_scaled, center=attr(data_train_scaled, "scaled:center"), 
                              scale=attr(data_testing_scaled, "scaled:scale")) 

data_train_true = data  %>% ungroup() %>% select(-c(cc_id, date, RET_1D_C,RET_1M_C)) %>% scale() %>% as.data.frame()

temp_test = data_testing %>% select(cc_id2, date_id, Id)
dtt_test = cbind(as.data.frame(temp_test), as.data.frame(data_test_scaled))


temp_mat = data %>% select(cc_id,date, RET_1D_C, RET_1M_C)
data_train_true = cbind(as.data.frame(temp_mat), as.data.frame(data_train_true))

sep_row = 8555

data_test_true = data_train_true[1:sep_row,] # 30.03.2019 and up
dtt = data_train_true[sep_row:nrow(data_train_true),]

var_names = colnames(data_train_true)
```

```{r}

for (i in 1:(ncol(dtt)-5)){
  Q1 = quantile(dtt[[i+5]], .25)
  Q3 = quantile(dtt[[i+5]], .75)
  IQR = IQR(dtt[[i+5]])
  no_outliers = subset(dtt, dtt[[i+5]] > (Q1 - 1.5*IQR) & dtt[[i+5]] < (Q3 + 1.5*IQR)) #throws away row outside q1 and q3 
}

dim(no_outliers)
```

```{r}

for (i in 1:(ncol(data_train_true)-5)){
  Q1 = quantile(data_train_true[[i+5]], .25)
  Q3 = quantile(data_train_true[[i+5]], .75)
  IQR = IQR(data_train_true[[i+5]])
  dtt2 = subset(data_train_true, data_train_true[[i+5]] > (Q1 - 1.5*IQR) & data_train_true[[i+5]] < (Q3 + 1.5*IQR)) #throws away row outside q1 and q3 
}

dim(dtt2)
```


```{r}
dtt2_temp = dtt2 %>% select(-c(1:5))
pca <- prcomp(dtt2_temp, scale. = T)

pca_train_data = data.frame(RET_1D = dtt2$RET_1D, pca$x)

dtt_test[is.na(dtt_test)] = 0
pca_test = dtt_test %>% ungroup() %>% select(-c(cc_id2, date_id, Id ))  %>% as.data.frame()
pca_test = prcomp(pca_test, scale. = T)

pca_temp_test <- predict(pca_test, newdata = dtt_test)
pca_temp_test <- as.data.frame(pca_temp_test)

```

```{r prep elasticnet, warning = FALSE, message = FALSE}
data_lasso <-  pca_train_data

y <- data_lasso$RET_1D                            # Dependent variable
x <- data_lasso %>%
  as.matrix()  

#x[is.na(x)] = 0 # setting NAs to Zero
#var_names = features_clean %>% as.vector() %>% t() %>% unlist() # Converting features clean matrix to Char vec
```


```{r, elasticnet}
fit <- glmnet(x,y, alpha = 0.07)                   # The elasticnet: 1 = Lasso, 0 = Ridge
res <- summary(fit$beta)                            # Summary of elasticnet regressions
lambda <- fit$lambda                                # Values of the penalisation constant
res$Lambda <- lambda[res$j]                         # Putting the labels where they belong
res$Char <- var_names[res$i] %>% as.factor()        # Adding the names of variables to the output
res %>% ggplot(aes(x = Lambda, y = x, color = Char)) + geom_line()
```

```{r}
data_test_pca <- predict(pca, newdata = data_test_true)
data_test_pca <- as.data.frame(data_test_pca)
```

```{r}
#install.packages("rpart")
library(rpart)
rpart.model <- rpart(RET_1D ~ .,data = pca_train_data, method = "anova")
rpart.model
```

```{r}
h2o.init()
var_names = c("RET_1D","PC34", "PC12", "PC23", "PC18", "PC27", "PC35", "PC7")#

dtt_test_h2o<-as.h2o(data_test_pca[1:125]) 
dtt_test_fin_h2o = as.h2o(data_test_pca)

smp_size <- floor(0.75 * nrow(pca_train_data))
## set the seed to make your partition reproducible
set.seed(5)
train_ind <- sample(seq_len(nrow(pca_train_data)), size = smp_size)
train_df <- pca_train_data[train_ind, ]
test_df <- pca_train_data[-train_ind, ]
# initialize the h2o
# create the train and test h2o data frames
train_df_h2o<-as.h2o(train_df %>% select(var_names))
test_df_h2o<-as.h2o(test_df %>% select(var_names))
 
y <- "RET_1D"
x <- setdiff(names(train_df_h2o), y)

```



```{r}

 nfolds <- 5
 
 learn_rate_opt <- c(0.001, .002, 0.003, 0.005, .007, .009, .01, .02,.03,.05,.07,.08)
 max_depth_opt <- c(9, 10, 12, 13 ,14, 15)
 sample_rate_opt <- c(0.7, 0.8, 0.9, 1.0)
 col_sample_rate_opt <- c(0.6, .65, 0.7, .75, 0.8)
  
  hyper_params <- list(learn_rate = learn_rate_opt,
                       max_depth = max_depth_opt,
                       sample_rate = sample_rate_opt,
                       col_sample_rate = col_sample_rate_opt)
  
  search_criteria <- list(strategy = "RandomDiscrete",
                          max_models = 100,
                          seed = 5)
  
  gbm_grid <- h2o.grid(algorithm = "gbm",
                       grid_id = "gaussian",
                       x = x,
                       y = y,
                       training_frame = train_df_h2o,
                       ntrees = 100,
                       seed = 5,
                       nfolds = nfolds,
                       keep_cross_validation_predictions = TRUE,
                       hyper_params = hyper_params,
                       search_criteria = search_criteria)
 
 ens_gbm_grid = h2o.stackedEnsemble(x = x,
                                    y = y,
                                    training_frame = train_df_h2o,
                                    validation_frame = test_df_h2o,
                                    base_models = gbm_grid@model_ids)
 
perf <- h2o.performance(ens_gbm_grid, newdata = test_df_h2o)
.getauc <- function(mm) h2o.mse(h2o.performance(h2o.getModel(mm), newdata = test_df_h2o))
baselearner_aucs <- sapply(gbm_grid@model_ids, .getauc)
baselearner_best_auc_test <- max(baselearner_aucs)
ensemble_auc_test <- h2o.mse(perf)
print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))

gbm_grid
print(ens_gbm_grid@model[["model_summary"]])

```

```{r}
 nfolds <- 5

 lambda  = c(0.0,0.01, 0.05, 0.07, 0.1, 0.15, 0.2, 0.3, 0.4, .5, .6, .7)
 alpha = c(0.0,0.01, 0.05, 0.07, 0.1, 0.15, 0.2, 0.3, 0.4, .5, .6, .7)

 hyper_params <- list(lambda  = lambda, alpha = alpha )
  
 search_criteria <- list(strategy = "RandomDiscrete",
                          max_models = 50,
                          seed = 5)
  
 glm_grid <- h2o.grid(algorithm = "glm",
                       grid_id = "gaussian",
                       x = x,
                       y = y,
                       training_frame = train_df_h2o,
                       seed = 5,
                       nfolds = nfolds,
                       keep_cross_validation_predictions = TRUE,
                       hyper_params = hyper_params,
                       search_criteria = search_criteria)
 
ens_glm_grid = h2o.stackedEnsemble(x = x,
                                  y = y,
                                  training_frame = train_df_h2o,
                                  validation_frame = test_df_h2o,
                                  base_models = glm_grid@model_ids)
 
 perf <- h2o.performance(ens_glm_grid, newdata = test_df_h2o)
 .getauc <- function(mm) h2o.mse(h2o.performance(h2o.getModel(mm), newdata = test_df_h2o))
 baselearner_aucs <- sapply(glm_grid@model_ids, .getauc)
 baselearner_best_auc_test <- max(baselearner_aucs)
 ensemble_auc_test <- h2o.mse(perf)
 print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
 print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
 
 glm_grid
```

```{r}

 nfolds <- 5

 ntrees  = c(20,30,40)
 max_depth = c(2:5)
   

 hyper_params <- list(ntrees  = ntrees, max_depth = max_depth )
  
 search_criteria <- list(strategy = "RandomDiscrete",
                          max_models = 5,
                          seed = 5)
  
rf_grid <- h2o.grid(algorithm = "drf",
                       grid_id = "gaussian",
                       x = x,
                       y = y,
                       training_frame = train_df_h2o,
                       seed = 5,
                       nfolds = nfolds,
                       keep_cross_validation_predictions = TRUE,
                       hyper_params = hyper_params,
                       search_criteria = search_criteria)
 
ens_rf_grid = h2o.stackedEnsemble(x = x,
                                  y = y,
                                  training_frame = train_df_h2o,
                                  validation_frame = test_df_h2o,
                                  base_models = rf_grid@model_ids)
 
 perf <- h2o.performance(ens_rf_grid, newdata = test_df_h2o)
 .getauc <- function(mm) h2o.mse(h2o.performance(h2o.getModel(mm), newdata = test_df_h2o))
 baselearner_aucs <- sapply(rf_grid@model_ids, .getauc)
 baselearner_best_auc_test <- max(baselearner_aucs)
 ensemble_auc_test <- h2o.mse(perf)
 print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
 print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
 
 rf_grid
```

```{r}

nfolds <- 5
# 1. Generate a 3-model ensemble (GBM + RF + Logistic)
# Train & Cross-validate a GBM
my_gbm <- h2o.gbm(x = x,
                  y = y,
                  training_frame = train_df_h2o,
                  nfolds = nfolds,
                  distribution = "gaussian",
                  ntrees = 100,
                  max_depth = 15,
                  col_sample_rate = 0.8,
                  sample_rate = 0.7,
                  #min_rows = 2,
                  learn_rate = 0.2,
                  keep_cross_validation_predictions = TRUE,
                  seed = 5)
# Train & Cross-validate a RF
my_rf <- h2o.randomForest(x = x,
                          y = y,
                          training_frame = train_df_h2o,
                          nfolds = nfolds,
                          max_depth = 2,
                          ntrees = 30,
                          validation_frame = test_df_h2o,
                          keep_cross_validation_predictions = TRUE,
                          seed = 5)

my_dl <- h2o.deeplearning(x = x,
                       y = y,
                       distribution = "gaussian",
                       hidden = c(1),
                       nfolds = nfolds,
                       epochs = 128,
                       train_samples_per_iteration = -1,
                       reproducible = TRUE,
                       activation = "Tanh",
                       balance_classes = FALSE,
                       seed = 5,
                       score_training_samples = 0,
                       score_validation_samples = 0,
                       training_frame = train_df_h2o,
                       validation_frame = test_df_h2o,
                       keep_cross_validation_predictions = TRUE,
                       stopping_rounds = 0)
# my_lr <- h2o.glm(x = x,
#                   y = y,
#                   training_frame = train_df_h2o,
#                   family = c("gaussian"),
#                   nfolds = nfolds,
#                   alpha = .0,
#                   lambda = .1,
#                   keep_cross_validation_predictions = TRUE,
#                   seed = 5)


# my_svm = h2o.psvm(gamma = 0.01,
#                   rank_ratio = 0.1,
#                   x = x,
#                    y = y,
#                    kernel_type = "gaussian",
#                    training_frame = train_df_h2o,
#                    validation_frame = test_df_h2o,
#                    disable_training_metrics = FALSE)

# No Windows support #####
# my_xgb = h2o.xgboost(x = x,
#                     y = y,
#                     training_frame = train_df_h2o,
#                     validation_frame = test_df_h2o,
#                     booster = "dart",  
#                     normalize_type = "tree",
#                     seed = 5)

# Train a stacked random forest ensemble using the GBM, RF and LR above
ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                metalearner_algorithm="naivebayes",
                                training_frame = train_df_h2o,
                                validation_frame = test_df_h2o,
                                base_models = list(my_gbm, my_rf, my_dl))

# Eval ensemble performance on a test set
perf <- h2o.performance(ensemble, newdata = test_df_h2o)
# Compare to base learner performance on the test set
# perf_my_svm_test <- h2o.performance(my_svm, newdata = test_df_h2o)
perf_gbm_test <- h2o.performance(my_gbm, newdata = test_df_h2o)
#perf_xgb_test <- h2o.performance(my_xgb, newdata = test_df_h2o)
#perf_ensemble_2_test = h2o.performance(ensemble_2, newdata = test_df_h2o)
#perf_auto_test <- h2o.performance(my_auto, newdata = test_df_h2o)
perf_rf_test <- h2o.performance(my_rf, newdata = test_df_h2o)
perf_dl_test <- h2o.performance(my_dl, newdata = test_df_h2o)

#perf_lr_test <- h2o.performance(my_lr, newdata = test_df_h2o)
#perf_xgb_test <- h2o.performance(my_xgb, newdata = test_df_h2o)
baselearner_best_auc_test <- max(h2o.mse(perf_gbm_test), h2o.mse(perf_rf_test), h2o.mse(perf_dl_test))#, h2o.mse(perf_lr_test))#, h2o.mse(perf_xgb_test))
ensemble_auc_test <- h2o.mse(perf)
print(sprintf("Best Base-learner Test MSE:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test MSE:  %s", ensemble_auc_test))

#pred <- h2o.predict(ensemble, newdata = test_df_h2o) # This is testing on the small test set. 

new_h2o = as.h2o(pca_temp_test)
pred2 <- h2o.predict(ensemble, newdata = new_h2o) # Big test set
#pred
pred2 = pred2 %>% as.data.frame()
```


```{r}
PF <- as.data.frame(dtt_test) %>% select(Id,cc_id2,date_id) %>% add_column(return=pred2) %>%
    arrange(Id) %>% group_by(date_id) %>% mutate(weight=as.numeric(return>=quantile(return,0.7,na.rm=TRUE)),weight=weight/sum(weight)) %>%
    ungroup() %>%
    select(Id,Predicted=weight)
    #select(Id,Predicted=weight, date_id)
 #PF$date_id = gsub(" ", "_", PF$date_id)
 #day_test = PF %>% filter(date_id =="Day_2")
 #sum(day_test$Predicted)

glimpse(PF)
readr::write_csv(PF,file="tuned_auto_ens.csv")
```
